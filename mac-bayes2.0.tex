\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arZiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[final]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography



\usepackage{amsthm}       % blackboard math symbols
\usepackage{amssymb}       % blackboard math symbols
\usepackage{mathtools}       % blackboard math symbols
\input{declarations.tex}
\input{packages.tex}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{comment}
\usepackage{bm}
\usepackage{upgreek}
\setlength{\marginparwidth}{10ex} 
\setlength{\marginparsep}{5mm}
\usepackage{todonotes}
\usepackage{bbding}
\usepackage{booktabs}
\usepackage{footnote}
\makesavenoteenv{tabular}
\makesavenoteenv{table}
\let\norm\undefined
\let\set\undefined
\let\floor\undefined
\let\ceil\undefined
\usepackage{notation}
\usepackage{bm}
\usetikzlibrary{patterns,snakes}
\DeclareMathOperator{\sub}{\mathscr{G}}
\let\footnotetitle\thanks
\DeclareRobustCommand{\VAN}[3]{#2} % proper Dutch 'van/de' capitalisation
\usepackage[mathscr]{euscript}
\renewcommand{\cN}{\mathscr{N}}
\newcommand{\drel}{\ensuremath{d_{\text{\sc rel}}}}
\newcommand{\relset}{\ensuremath{{\cal D}_{\text{\sc rel}}}}
\newcommand{\srel}{\ensuremath{\sigma^2_{\text{\sc rel}}}}
\newcommand{\sirrel}{\ensuremath{\sigma^2_{\text{\sc irr}}}}
\makeatother


\renewcommand{\P}{\mathbf{P}}
\newcommand{\ro}{\uprho}
\newcommand{\hro}{\what{\uprho}}
\renewcommand{\erm}{\textsc{erm}}

\makeatletter
\renewenvironment{proof}[1][\proofname]{\par
	\pushQED{\qed}%
	\normalfont \topsep6\p@\@plus6\p@\relax
	\trivlist
	\item[%
	\hskip\labelsep
	\normalfont\bfseries % was \itshape
	#1%
%	\@addpunct{}% remove this if you don't want punctuation
	]\ignorespaces
}{%
	\popQED\endtrivlist\@endpefalse
}
\let\qed\relax % avoid a warning
\DeclareRobustCommand{\qed}{%
	\ifmmode \mathqed
	\else
	\leavevmode\unskip\penalty\@M\hbox{}\nobreak\hfill%\hspace{.5em minus .1em}% was 
	\hbox{\qedsymbol}%
	\fi
}
\makeatother
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}




\title{Second Bound}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeZ to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeZ puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
	% \And
	% Coauthor \\
	% Affiliation \\
	% Address \\
	% \texttt{email} \\
	% \AND
	% Coauthor \\
	% Affiliation \\
	% Address \\
	% \texttt{email} \\
	% \And
	% Coauthor \\
	% Affiliation \\
	% Address \\
	% \texttt{email} \\
	% \And
	% Coauthor \\
	% Affiliation \\
	% Address \\
	% \texttt{email} \\
}

\begin{document}
	


\section{Preliminaries}
For variables $Z_1,\dots, Z_n$ in some set $\cZ$, we denote $Z^n \coloneqq (Z_1,\dots,Z_n)\in \cZ^{n}$, and for $i \in[n]$,
\begin{align}
Z^{n}_{\smallsetminus i} \coloneqq (Z_1,\dots, Z_{i-1}, Z_{i+1}, \dots, Z_n) \in \cZ^{n-1}. 
\end{align} 
Given a loss function $\ell\colon \cH \times \cZ \rightarrow \reals$, we denote by $L(h) \coloneqq \E_{z\sim P_Z}[\ell(h,z)]$, for $h\in \cH$, where $P_Z$ is the marginal distribution of $Z$. For any distribution $\ro$ on $\cH$, we write $L(\ro)\coloneqq \E_{h\sim \ro}[L(h)]$. We denote by $\what L(h)$ and $\what L(\ro)$ the empirical counterparts of $L(h)$ and $L(\ro)$, respectively.


\section{Main Bounds}
\begin{theorem}
	\label{thm:KL}
	If the loss $\ell$ takes values in $[0,1]$, then for any posterior $ \ro \colon \bigcup_{i=1}^n \cZ^i \rightarrow \triangle( \cH)$, with $\hro_i \coloneqq  \ro(Z^n_{\smallsetminus i})$ and $\hro=  \ro(Z^n) $, we have
	\begin{align}
	{\bf E}_{Z^n}  \left[ \kl(\what{L}(\hro) \| L(\hro))\right] \leq 
	{\bf E}_{Z^n } \left[\frac{
		\sum_{i=1}^n \KL(\hro \| \hro_i)+1}{n} \right],
	\end{align}
\end{theorem}

\begin{theorem}
	\label{thm:VP}
	Let $\eta\in]0,1[$ and $\vartheta(\eta) := 
	(- \ln (1-\eta) - \eta)/\eta^2$. If the loss $\ell$ takes values in $[0,1]$, then for any posterior $ \ro \colon \bigcup_{i=1}^n \cZ^i \rightarrow \triangle( \cH)$, with $\hro_i \coloneqq  \ro(Z^n_{\smallsetminus i})$ and $\hro=  \ro(Z^n) $, we have
	\begin{gather} \label{eq:bound}
	\E_{Z^n}\left[ L(\hro) - \what L(\hro)\right] \leq  \E_{Z^n}  \left[\eta \vartheta(\eta) \what V(\hro) + \frac{\sum_{i=1}^n\KL(\hro \| \hro_{i})}{\eta \cdot  n}  \right],\\
	\text{where} \quad \quad  \what V(\hro) \coloneqq  \frac{1}{n} \sum_{i=1}^n \E_{h\sim \hro} \left[ \left(\ell(h, Z_i) - \E_{h'\sim \hro_i}[\ell(h',Z_i)]\right)^2 \right]. \label{eq:theV}
	\end{gather}
\end{theorem}

\section{Comparing LOO-KL with Full-KL}
In this section, we compare the magnitude of the Leave-One-Out $\KL$ (LOO-KL) and the full $\KL$ in two different settings. We will make use of the following well-known fact about the $\KL$-divergence between two multivariate Gaussian distributions $P \equiv \cN(\mu_P, \Sigma_P)$ and $Q\equiv \cN(\mu_Q, \Sigma_Q)$:
\begin{align}
\KL(P \| Q) = \frac{1}{2} \left(\log \frac{| \Sigma_Q |}{|\Sigma_P|} - d + \text{\sc trace}\left( \Sigma_Q^{-1} \Sigma_P \right)    + (\mu_Q-\mu_P)^{\transpose} \Sigma_Q^{-1} (\mu_Q - \mu_P) \right). \label{eq:KLexpress}
\end{align}
 \subsection{Density Estimation with a Multivariate Gaussian Statistical Model}
Consider i.i.d.~random vectors $Z_1, \ldots Z_n \in \reals^d$ and a prior/posterior on a multivariate normal
statistical model, $\{P_{\mu} : \mu \in \reals^d \}$ with diagonal
covariance matrix $\Sigma = \sigma^2 {I}_d$, for some fixed $\sigma^2 > 0$. We will assume further that the model is correct, \emph{i.e.} the $Z_i$ are i.i.d.~according to some $p_{\mu}$ with unknown $\mu$.


To analyze the multivariate KL, we first note some immediate properties for the univariate case; for $m \in \mathbb{N}$, let $\hat{\mu}_m$ be the maximum likelihood estimator based on i.i.d.~$X_1, \ldots, X_m\in \reals$ distributed as $N(\mu,\sigma^2)$. 
We trivially have  $\hat{\mu}_{n} = X_{n}/n + ((n-1)/n) \hat{\mu}_{n-1}$ so that, after rearranging,
\begin{equation}\label{eq:mlebasics0}
\hat{\mu}_n - \hat{\mu}_{n-1} = \frac{1}{n} \cdot \left( X_n -  \hat{\mu}_{n-1}\right).
\end{equation}  
Also, we have
\begin{align}\label{eq:mleadvanced0}
{\bf E}_{X \sim P_{\mu}}\left[(X_n - \hat{\mu}_{n-1})^2\right]
& =
{\bf E}_{X \sim P_{\mu}}\left[((X_n - \mu) + (\mu - \hat{\mu}_{n-1}))^2\right] 
\nonumber\\ & =  {\bf E}_{X \sim P_{\mu}}\left[(X_n - \mu)^2 + (\mu - \hat{\mu}_{n-1})^2\right]
\\ &= \sigma^2 + \frac{\sigma^2}{n-1} = \frac{n}{n-1} \cdot \sigma^2.
\end{align}
where the second equality follows by independence of $X_n$ and $\hat{\mu}_{n-1}$ and the fact that ${\bf E}[X_n] = \mu$. 

Now we go back to the multivariate case; $\hat{\mu}_m \coloneqq (\hat \mu_{m,1}, \dots, \hat\mu_{m,d})\in \reals^d$ now stands for the MLE/least squares estimator; that is, $\hat\mu_m = \frac{1}{m} \sum_{i=1}^m Z_i$.

We will now apply this to our model, with $\hro^m= \hro(Z^n)$ a distribution
centered around the ERM $\hat{\mu}_m$ with diagonal covariance matrix
with entries $\gamma_m \sigma^2$. First, define the function
$g\colon \reals_{>0} \rightarrow \reals$ by $g(u) = - \log u + u-1$ and
note that $g(1) = 0$, (by differentiation) $g(u) > 0$ if $u \neq 1$
and, for integer $n$, $g((n-1)/n) = O(1/n^2)$.

\paragraph{The LOO-KL case.} Let $m=n \in \mathbb{N}$ and define $\alpha \coloneqq \gamma_n/\gamma_{n-1}$. From \eqref{eq:KLexpress}, we get
\begin{align}
\KL(\hro^n \| \hro^{n-1}) & = \frac{1}{2} \left(\log \frac{| (\gamma_{n-1} \cdot \sigma^2)^d |}{
	(\gamma_n \sigma^2)^d} - d + d
\cdot \frac{\gamma_n}{\gamma_{n-1}}
+ \sum_{j=1}^d \frac{(\hat{\mu}_{n,j} - \hat{\mu}_{n-1,j})^2}{\gamma_{n-1} \sigma^2}\right) \nonumber \\ 
& =  \frac{1}{2} \left( d \cdot g(\alpha) + \sum_{j=1}^d \frac{(\hat{\mu}_{n,j} - \hat{\mu}_{n-1,j})^2}{ \gamma_{n-1} \sigma^2}\right) \nonumber \\
& =  \frac{1}{2} \left( d \cdot g(\alpha) + \sum_{j=1}^d \frac{(X_{n,j} - \hat{\mu}_{n-1,j})^2}{n^2 \cdot \gamma_{n-1} \sigma^2}\right),                       \nonumber
\end{align}
where in the first inequality we used (\ref{eq:mlebasics0}).
We thus have, using (\ref{eq:mleadvanced0}), 
\begin{align*}
{\bf E}_{Z^n \sim P_{\mu}}  \left[  \KL(\hro^n \| \hro^{n-1})  \right]   =  \frac{d}{2} \left(
g(\alpha) + \frac{1}{ \gamma_{n-1} \cdot n \cdot (n-1)}\right)
\end{align*}
so that for $\hro \coloneqq \hro^n= \hro(Z^n)$ and $\hro_i \coloneqq \hro(Z^{n}_{\smallsetminus i})$, we get 
\begin{align}
{\bf E}_{Z^n \sim P_{\mu}}  \left[  \sum_{i=1}^n \KL(\hro \| \hro_{i}) \right]
=  \frac{d}{2} \left(n \cdot g(\alpha)  +
\frac{1}{\gamma_{n-1}} \cdot \frac{1}{n-1} \right)
\label{eq:sumKLs0}
\end{align}
We note that, for given $\gamma_n$, this is minimized if we take
$\gamma_n = \gamma_{n-1}$ so that $\alpha=1$ and $g(\alpha)=0$.

\paragraph{The Full-KL case.} Now, a similar analysis for the classic $\KL(\hro \| \ro_0)$ for proper prior $\ro_0$ centered at the {\em correct\/} $\mu$ and diagonal covariance matrix with entries $\sigma^2$, shows that it is of order $d (\log n)$; if the prior $\ro_0$ is centered at the wrong $\mu$, it can only get larger.
\commentout{ If we take
the `real' Bayesian posteriors (explained below), we would get $\gamma_{n-1} = 1/{n-1}$
and $\gamma_n = 1/n$. This might {\em not\/} be the optimal choice for
our purposes --- since by having $\gamma_{n-1}$ larger (say,
$1/\sqrt{n}$?) we may be able to deal with dimensionality $d \gg n$.
But it does make the term $n g(\alpha)$ equal to
$n g((n-1)/n) = O(1/n)$, so that one remains negligible.



Now let's contrast this with the case where we base the prior on the first half of the sample, as in our Neurips 2019 paper. Denoting the MLE based on the second half of the sample $Z_{n/2+1}, \ldots, Z_n$  by $\hat{\mu}_{n/2+1:n}$, and now setting $\alpha := \gamma_n/\gamma_{n/2}$, we then  get 
\begin{align}\label{eq:forlater0}
\KL(\hat{\Pi}_n \| \hat{\Pi}_{n/2}) & = 
\frac{1}{2} \left(d
\cdot g(\alpha)
+ \sum_{j=1}^d \frac{(\hat{\mu}_{n/2+1:n,j} - \hat{\mu}_{n/2,j})^2}{4 \gamma_{n/2} \cdot \sigma^2}\right)
\end{align}
by a similar calculation as above. Taking expectations now gives for the 2-fold complexity: 
\begin{align*}
& {\bf E}_{Z^n \sim P_{\mu}} \left[\sum_{k=0}^1 \KL(\hat{\Pi}_n \| \hat{\Pi}_{|Z_{k \cdot n/2 +1 }, \ldots, Z_{k \cdot n/2 + n/2}})\right] =    2 {\bf E}_{Z^n \sim P_{\mu}} \left[\KL(\hat{\Pi}_n \| \hat{\Pi}_{n/2})\right] \\
& = d \cdot g(\alpha) + \sum_{j=1}^d {\bf E}_{Z^n \sim P_{\mu}} \left[\frac{
	((\hat{\mu}_{n/2+1:n,j} - \mu ) + (\mu-  \hat{\mu}_{n/2,j}))^2}{4 \gamma_{n/2} \cdot \sigma^2}\right] \\& =  d \cdot
\left(g(\alpha)  +
\frac{1}{\gamma_{n/2}} \cdot \frac{1}{n}\right).
\end{align*}
Note that the dependence on $g(\alpha)$ is much better (no factor $n$); but this doesn't matter since for the optimal $\alpha$ $g(\alpha) =0$ and the term falls out; we then see that the two-fold KL is approximately a factor of 2 larger than the LOO-KL. 



So we do find a {\em significant\/} advantage of using the two-fold KL rather than the standard KL (order $d$ vs $d \log n$) and another {\em significant\/} advantage of using the leave-one-out KL rather than the two-fold KL (order $1 + d/n$ rather than order $d$).}

\section{Detailed Analysis}
\paragraph{ESI Notation.} Let $\eta>0$, and $X$, $Y$ be any two random variables. We define
	\begin{align}\label{eq:esi}
	X \stochleq_{\eta} \  Y  \ \ \iff \ \ X -Y\stochleq_{\eta} \  0  \ \ \iff \ \ \E \left[e^{\eta (X- Y)} \right] \leq 1.
	\end{align}
%We can extend the above definition to the case where $\eta=\hat{\eta}$ is also a random variable, in which case the expectation in \eqref{eq:esi} needs to be replaced by the expectation over the joint distribution of ($X$, $Y$, $\hat\eta$). When no ambiguity can arise, we omit $D$ from the ESI notation. 
When $\eta=1$, we simply write $X \stochleq Y$ (\emph{i.e.}~we ommit the subscript $\eta$). Besides simplifying notation, ESIs are useful in that they  simultaneously capture ``with high probability'' and ``in expectation'' results (the proofs of the next three propositions can be found in \citep{mhammedi2019pac}):
%\begin{proposition}{\bf  [ESI Implications]} \label{prop:drop} 
%	For fixed $\eta > 0$, if $X \stochleq_{\eta} Y$ then $\E [X] \leq \E[Y]$. 
%	For both fixed and random $\hat\eta$, if $X \stochleq_{\hat\eta} Y$, then $\forall \delta \in]0,1[$, $X\leq Y + (-\ln \delta)/\hat \eta$, with probability at least $1-\delta$.
%\end{proposition}
\begin{proposition} \label{prop:drop} 
	For fixed $\eta > 0$, if $X \stochleq_{\eta} Y$ then (a) $\E [X] \leq \E[Y]$; and (b) for all $\delta \in]0,1[$, $X\leq Y + (-\ln \delta)/ \eta$, with probability at least $1-\delta$.
\end{proposition}
The next result is on the additivity property of ESI: 
\begin{proposition}\label{prop:Trans} 
 Let $Z_1, \ldots, Z_n$ be i.i.d.~random variables in $\cZ$, and let $\Psi \colon \cZ \rightarrow \reals$ be any real-valued function. If for some $\eta>0$, $\Psi(Z_i) \stochleq_\eta 0$, for all $i \in [n]$, then $\sum_{i=1}^n \Psi(Z_i) \stochleq_{\eta} 0$.
\end{proposition}
We now give the Donsker-Varadhan variational formula expressed in ESI form: \begin{proposition} \label{prop:donsker}
	Fix $\eta > 0$ and let $\{X^h: h \in \cH\}$ be any family of random variables such that for all $h \in \cH$, $X^h \stochleq_{\eta} 0$. Let $\ro_0$ be any prior distribution on $\cH$ and let $\ro \colon \bigcup_{i=1}^n \cZ^{i} \rightarrow \triangle(\cH)$ be a learning algorithm.  We have, for $\hro = \ro(Z^n)$,
	\begin{align} \E_{h \sim \hro}[X^{h}] \stochleq_{\eta} \frac{\KL(\hro \| \ro_0)}{\eta}. \label{eq:PACBayes} \end{align} 
\end{proposition}
The following lemma will be key in the proofs of our bounds:
\begin{lemma}
	\label{lemma:mainlem}
	Let $\{\Eff^{  h}_{i}\colon \cZ^n \rightarrow \reals \mid i\in[n], h \in  \cH \}$ be any collection of random variables satisfying, \begin{align}  \Eff^{h}_{i}(z_1,\dots, z_{i-1}, Z_i, z_{i+1},\dots, z_n) \stochleq 0,\quad \text{for all $i\in[n]$, $h \in  \cH$, and $z^n_{\smallsetminus i} \in \cZ^{n-1}$.} \label{eq:assum00}\end{align}
	Then, for any posterior $  \ro \colon \bigcup_{i=1}^n \cZ^i \rightarrow \triangle( \cH)$ with $\hro_i \coloneqq  \ro(Z^n_{\smallsetminus i})$, $i\in [n]$, and $\hro=  \ro(Z^n)$:
	\begin{align}
	\E_{Z^n}\left[ \frac{1}{n}\sum_{i=1}^n  \E_{ h \sim \hro}  \left[ \Eff^{  h}_{i}(Z^n)\right]\right] \leq 	 \E_{Z^n}\left[ \frac{\sum_{i=1}^n\KL(\hro \| \hro_{i})}{ n} \right].
	\end{align}
\end{lemma}
\begin{proof} Let $i\in[n]$ and $Z^{(i)}_1, \dots, Z^{(i)}_n$ be i.i.d.~copies of $Z_1, \dots, Z_n$. Applying Proposition~\ref{prop:donsker} to \eqref{eq:assum00} with $i\in [n]$, prior $\hro^{(i)}_{i} \coloneqq  \hro(Z^{(i)}_1, \dots, Z^{(i)}_{i-1},Z^{(i)}_{i+1}  ,\dots, Z^{(i)}_n)$, and posterior $\hro^{(i)}\coloneqq \hro(Z^{(i)}_1, \dots,Z^{(i)}_n)$ on $ \cH$, we get 
	\begin{align}
	\E_{  h \sim \hro^{(i)}} \left[ \Eff_i^{  h}(Z^{(i)}_1, \dots, Z^{(i)}_n) \right]- \KL(\hro^{(i)}\|\hro_i^{(i)}) \stochleq 0. \label{eq:mid}
	\end{align}
	since $Z^{(i)}_j$ are i.i.d.~random variables, we can use Proposition~\ref{prop:Trans} to sum \eqref{eq:mid} for $i\in[n]$. This leads to 
	\begin{align}
	\sum_{i=1}^n	\E_{  h \sim \hro^{(i)}} \left[ \Eff_i^{  h}(Z^{(i)}_1, \dots, Z^{(i)}_n) \right]-\sum_{i=1}^n \KL(\hro^{(i)}\|\hro_i^{(i)}) \stochleq 0.
	\end{align}
	By applying Lemma \ref{prop:drop}-(a), we get 
	\begin{align}
	0 &\geq \E\left[ \sum_{i=1}^n \left(	\E_{ h \sim \hro^{(i)}} \left[ \Eff_i^{  h}(Z^{(i)}_1, \dots, Z^{(i)}_n) \right] - \KL(\hro^{(i)}\|\hro_i^{(i)})  \right)  \right], \\
	& = n  \E_{Z^n}  \left[	\E_{h \sim   \hro} \left[ \Eff_i^{  h}(Z_1, \dots, Z_n) \right]\right]   - \E_{Z^n}  \left[\sum_{i=1}^n \KL(\hro\|\hro_i)  \right], \label{eq:goal}
	\end{align}
	where the last equality follows by the fact that
	\begin{align*}
	\E_{Z^n}\left[\E_{ h \sim \hro^{(i)}} \left[ \Eff_i^{  h}(Z^{(i)}_1, \dots, Z^{(i)}_n)\right] - \KL(\hro^{(i)}\|\hro_i^{(i)})   \right] =  \E_{Z^n}\left[	\E_{ h \sim   \hro} \left[ \Eff_i^{  h}(Z^n) \right] - \KL(\hro\|\hro_i)   \right], 
	\end{align*}
	for all $i\in[n]$, since $Z^{(i)}_1, \dots, Z^{(i)}_n$ are i.i.d.~copies of $Z_1, \dots, Z_n$. Rearranging \eqref{eq:goal} and dividing by $n$ leads to the desired result.
\end{proof}

\commentout{
	In our main result (Theorem \ref{thm:main}), we present a PAC-Bayesian bound which holds in expectation. To obtain such a result, we need a way to go from an ESI inequality with a random $\eta$ to an inequality which holds in expectation. The next proposition provides us with the means to do this:  
	\begin{proposition}{\bf [From ESI to \Exp]}
		\label{prop:esitoexp}
		Let $\cG$ be a countable subset of $\reals_{>0}$ such that $\sup \cG <\infty$ and $\inf \cG >0$. Given a collection of random variables $\{Y_\eta: \eta \in \cG \}$ and an estimator $\estimate\eta$ with support on $\cG$ satisfying $Y_{\estimate\eta} \stochleq_{\estimate\eta} 0$, we have
		\begin{equation}\label{eq:traina}
		\expect{}{Y_{\estimate\eta}} \leq \expect{}{
			\frac{1 +  \ln \frac{\sup \cG}{\inf\cG} }{\estimate\eta}
		}.
		\end{equation}
	\end{proposition}
}
\subsection{Proof of Theorem \ref{thm:VP}}
The following lemma due to \citep{mhammedi2019pac} is central to the proof of Theorem \ref{thm:VP}. %Part (a) is not really new but a slight variation of existing results found in \cite{howard2018uniform,fan2015exponential}.            
\begin{lemma}{\bf \ [{\em un-\/}expected Bernstein]}
	\label{lem:bernsand}
	Let $X$ be a random variable bounded from {\em above} by $1$ almost surely, and let $\vartheta(u) := 
	(- \ln (1-u) - u)/u^2$. For all $0 < \eta < 1$, we have:
	\begin{align}\label{eq:blade}
	\E[X] - X  \stochleq_{\eta} \ \eta \vartheta(\eta) \cdot X^2.
	\end{align} 
\end{lemma}
We now prove Theorem \ref{thm:VP}:
\begin{proof}{\bf{of Theorem \ref{thm:VP}}}
	Let $\eta \in]0,1[$. For $i\in[n]$, $z^n_{\smallsetminus i} \in \cZ^{n-1}$, and $h\in \cH$, let $$\ro_i \coloneqq \ro(z^n_{\smallsetminus i})\quad  \text{and} \quad  g^h(Z_i, z^{n}_{\smallsetminus i}) \coloneqq \eta \ell(h,Z_i) -\eta  \E_{h\sim \ro_i } [\ell(h, Z_i)].$$
	Furthermore, let
	\begin{align*}
	\Phi^{h}_i(z_1, \dots, z_{i-1}, Z_i, z_{i+1}, \dots,z_n) & \coloneqq \E [ g^h(Z_i, z^{n}_{\smallsetminus i}) ] - g^h(Z_i, z^{n}_{\smallsetminus i})   - \eta \vartheta(\eta) g^h(Z_i, z^{n}_{\smallsetminus i}) ^2.
	\end{align*}
	Lemma \ref{lem:bernsand} implies that 
	\begin{align}
	\Phi^{h}_i(z_1, \dots, z_{i-1}, Z_i, z_{i+1}, \dots,z_n)  \stochleq  0.
	\end{align} 
	By applying Lemma \ref{lemma:mainlem} and rearranging the resulting inequality, we get the desired result.
\end{proof}

\subsection{Proof of Theorem \ref{thm:KL}}
\begin{proof}
	We will first show that the inequality hold for $\ell\colon \cZ \rightarrow \{0,1\}$. 
For $h \in \cH$ and $i\in[n]$, let $X^h := \ell(h,Z)$ and
$X^{h}_i := \ell(h, Z_i)$; since $\ell \in \{0,1\}$, $X^h$ and $X^h_i$ can be viewed as Bernoulli random variables. For $h\in \cH$, let
$L(h) = \E [\ell(h,Z)]$ be the probabiltiy that
$h$ makes a mistake, \emph{i.e.}~$L(h) = \P[\ell(h,Z) = 1]= \P[X^h = 1]$. Let $p(\ \cdot\ ;{\mu})$ be the probability mass function of a Bernoulli distribution with parameter $\mu \in [0,1]$; that is,
$p(1; \mu) = \mu$ and $p(0;\mu)=1-\mu$. 

For $n >1$, $0 \leq s \leq n-1$, and $h \in \cH$, we have
\begin{align}{\bf E} \left[\frac{p(X^h; (s+X^h)/n)}{p(X^h; L(h))} \right]
= L(h) \cdot \frac{(s+1)/n}{L(h)} + 
(1- L(h)) \cdot \frac{(n-s)/n}{1-L(h)} = 1 + \frac{1}{n}. \label{eq:theinequality}
\end{align}
For $z^{n}_{\smallsetminus i} \in \reals^{n-1}$ and $L_i(h)\coloneqq  (\ell(h, Z_i) + \sum_{j \neq i} \ell(h, z_j))/n$, let
\begin{align}
\Phi^h_{i}(z_1,\dots,z_{i-1}, Z_i, z_{i+1}, \dots, z_n) \coloneqq \log \frac{p(X^h_{i}; L_i(h) )}{p(X^h_{i}; L(h))} - \log \left(1 + \frac{1}{n} \right).
\end{align}
From \eqref{eq:theinequality}, we have
$$
\Phi^h_{i}(z_1,\dots,z_{i-1}, Z_i, z_{i+1}, \dots, z_n) \stochleq  0.
$$
Thus, by applying Lemma \ref{lemma:mainlem}, we get
%Using Donsker-Varadhan-PAC-Bayes we now get:
%$$
%{\bf E}_{f \sim \hat{\Pi}_n} \left[\log \frac{p_{\hat{L}_h}(U_{f,n})}{p_{\tilde\mu_h}(U_{f,n})} \right] \stochleq^{P|Z^{n-1}}_1 \KL(\hat{\Pi}_n \| \hat{\Pi}_{{n-1}}) + \log \left(1 + \frac{1}{n} \right).
%$$
%Now using Lemma~\ref{lem:mainb} below (with $i$ in the role of $n$, for $i =1..n$) we see that we can apply it with all  $\xi_i=0$, and we get (note that $\log$ denotes natural logarithm):
\begin{align*}
\E_{Z^n}\left[ \E_{h \sim \hro} \sum_{i=1}^n \left[\log \frac{p(X^h_{i}; \what{L}(h))}{p(X^h_{i}; L(h))} \right] \right]& \leq  \E_{Z^n} \left[
\sum_{i=1}^n \KL(\hro \| \hro_{i}) \right]+ n \log \left(1 + \frac{1}{n} \right) \\
&
\leq \E_{Z^n} \left[
\sum_{i=1}^n \KL(\hro \| \hro_{i}) \right]+ 1,
\end{align*}
where we used the fact that $\log (1 +x) \leq x$.
Now, by the robustness property of KL divergence for exponential family (Peter's book, Chapter 19), this gives that:
\begin{align}
n  \E_{Z^n} \left[\E_{h \sim \hro} \left[ \kl(\what{L}(h) \| L(h))\right] \right] &\leq 
\E_{Z^n} \left[
\sum_{i=1}^n \KL(\hro \| \hro_{i}) \right]+ 1, \label{eq:thebefore}
\end{align}
We now show that this inequality holds for losses taking values in $[0,1]$ (rather than just in $\{0,1\}$). Let $N \in \mathbb{N}$ and $\cP \coloneqq \{\cZ_1, \dots, \cZ_N\}$ be a partition of $\cZ$; that is, $\cZ_1, \dots, \cZ_N$ are disjoint (measurable) subsets of $\cZ$ such that $\bigcup_{i=1}^N \cZ_i = \cZ$. Further, for $h\in \cH$, let $\bm{v}^h \coloneqq (v^h_1, \dots, v^h_N)^{\top} \in [0,1]^N$ such that $\ell(h, Z)\geq    \sum_{k=1}^N v^h_k \cdot \mathbbm{1}\{Z \in \cZ_k \}.$ We define
\begin{align}
X(\bm{v}^h; \cP) \coloneqq  \kl\left(\frac{1}{n} \left.\sum_{i=1}^n  \sum_{k=1}^N v^h_k \cdot \mathbbm{1}\{Z_i \in \cZ_k \} \right\| \E_Z\left[ \sum_{k=1}^N  v^h_k \cdot \mathbbm{1}\{Z \in \cZ_k \}\right]    \right).
\end{align}
By the joint convexity of $\kl$, $X( \cdot ; \cP)$ is convex and so its supremum is attained at the boundary of $[0,1]^N$ which is just $\{0,1\}^N$. Thus, for all $h\in \cH$, there exists $\bar{\bm{v}}^h \in\{0,1\}^{N}$ such that $X(\bm{v}^h; \cP)\leq X(\bar{\bm{v}}^h; \cP)$. By letting $\bar{\ell}(h, Z) \coloneqq  \sum_{k=1}^N \bar{v}^h_k \cdot \mathbbm{1}\{Z \in \cZ_k \}$, we get
\begin{align}
\E_{Z^n}\left[ \E_{h\sim \hro}\left[X(\bm{v}^h; \cP) \right]\right]  &\leq \E_{Z^n}\left[ \E_{h\sim \hro}\left[X(\bar{\bm{v}}^h;\cP) \right]\right]\nonumber \\ & =  \E_{Z^n}\left[\E_{h\sim \hro} \left[\kl\left(\frac{1}{n} \left.\sum_{i=1}^n \bar{\ell}(h, Z_i) \right\| \E_Z\left[\bar\ell(h,Z)\right]    \right) \right]\right] \nonumber \\
& \leq \E_{Z^n} \left[
\frac{\sum_{i=1}^n \KL(\hro \| \hro_{i})+ 1}{n} \right], \label{eq:mids}
\end{align}
where the last inequality follows by \eqref{eq:thebefore} since $\bar\ell(h,z)\in\{0,1\}$, for all $(h,z)\in \cH \times \cZ$. By definition of the Lebesgue integral, for any $h\in \cH$, there exists $(\bm{v}^{h}_{N} \in [0,1]^N)_{N \in \mathbb{N}}$ and a sequence of partitions $(\cP_{N}\coloneqq  \{ \cZ_{N,1},\dots, \cZ_{N,N} \})$ of $\cZ$, such that 
\begin{gather}
\ell(h, Z)\geq    \sum_{k=1}^N v^h_{N,k} \cdot \mathbbm{1}\{Z \in \cZ_{N,k} \}; \quad  \lim_{N \to \infty}   \sum_{k=1}^N  v^{h}_{N,k} \cdot \mathbbm{1}\{Z \in \cZ_{N,k} \} = \ell(h, Z) \\ \text{and} \quad \lim_{N \to \infty} \E_Z\left[ \sum_{k=1}^N  v^{h}_{N,k} \cdot \mathbbm{1}\{Z \in \cZ_{N,k} \}\right] = \E[\ell(h, Z)]
\end{gather}
Using this and the continuity of $\kl$ (which follows from its convexity), we get 
\begin{align}
\E_{Z^n}\left[ \E_{h\sim \hro}\left[\kl(\what{L}(h) \| L(h))\right]\right] &=\E_{Z^n}\left[ \E_{h\sim \hro}\left[\lim_{N\to \infty} X({\bm{v}}^{h}_N; \cP_N)  \right]\right].\label{eq:1}
\shortintertext{Since $X$ is non-negative ($\kl$ is non-negative), we have, by Fatou's Lemma:}
\E_{Z^n}\left[ \E_{h\sim \hro}\left[\lim_{N\to \infty} X({\bm{v}}^{h}_N; \cP_N)  \right]\right]&\leq  \liminf_{N \to\infty} \E_{Z^n}\left[ \E_{h\sim \hro}\left[X(\bm{v}^{h}_N; \cP_N) \right]\right], \\
& \leq \E_{Z^n} \left[
\frac{\sum_{i=1}^n \KL(\hro \| \hro_{i})+ 1}{n} \right], \label{eq:last}
\end{align}
where the last inequality follows by \eqref{eq:mids}. By combining \eqref{eq:1} and \eqref{eq:last}, and using the joint convexity of the $\kl$ for Bernoulli random variables, we get
\begin{align}
\E_{Z^n}  \left[ \kl(\what{L}(\hro) \| L(\hro))\right] &\leq 
\E_{Z^n} \left[
\frac{\sum_{i=1}^n \KL(\hro \| \hro_{i})+ 1}{n} \right]. \label{eq:ziel}
\end{align}
\end{proof}

\commentout{
\section{(In Progress) Fast Rate Excess-Risk Bounds for Convex Losses}
In this section, we show how our bound in Theorem \ref{thm:VP} allows us to easily recover fast excess-risk bounds for ERM when the loss function $\ell \colon \cH \times \cZ \rightarrow [-1,1]$ is strongly convex or exp-concave in the first argument. For this, we consider the following condition:
\begin{definition}
	Let $\beta \in[0,1]$ and $B>0$. The $(\beta,B)$-Bernstein condition holds if for all $h\in \cH$, \begin{align}\nonumber \expect{}{\left(\ell(h,X)- \ell(h^\star,X)\right)^2} \leq B \cdot \expect{}{\ell(h,X)- \ell(h^\star,X)}^{\beta},\end{align}
	where $h^\star \in \argmin_{h\in \cH} \expect{}{\ell(h,X)}$ is a risk minimizer within the closer of $\cH$.
\end{definition}
The Bernstein condition \cite{audibert2004pac,bartlett2006empirical,erven2015fast,koolen2016combining} essentially characterizes the ``easiness'' of the learning problem; it implies that the variance in the excess loss random variable $\ell(h,X)- \ell(h^\star,X)$ gets smaller the closer the risk of hypothesis $h\in \cH$ gets to that of the risk minimizer $h^\star$. For bounded loss functions, the BC with $\beta=0$ always holds. The Bernstein condition with $\beta=1$ (the ``easiest'' learning setting) is also known as the {\em Massart noise condition}; in particular, it holds whenever $\cH$ is convex and $h \mapsto \ell(h,z)$ is strongly convex or exp-concave, for all $z\in \cX$ \cite{erven2015fast,mehta2017fast}\footnote{For more examples of learning settings where a BC holds see \cite[Section 3]{koolen2016combining}.}. 

In what follows, we denote $\hat h^{\textsc{erm}} \in \argmin_{h \in \cH} \what L(h)$ the empirical risk minimizer.
\begin{theorem}
	\label{thm:bern}
	If $\ell\colon \cH \times \cZ \rightarrow \reals$ is convex in the first argument, and the $(\beta, B)$-Bernstein condition holds for $\beta \in[0,1]$ and $B>0$, then there exists a constant $C>0$ such that
	\begin{align}
	\E_{Z^n}\left[ L(\hat h^{\textsc{erm}}) - L(h^\star)\right] \leq \frac{C}{n} \label{eq:exp}
	\end{align}
\end{theorem}
Using the boosting the confidence trick due to \cite[Theorem~4]{mehta2017fast}, a fast rate high-probability bound can be extracted from \eqref{eq:exp}. We now show how a fast $O(1/n)$ excess-risk bound for ERM follows from Theorem \ref{thm:bern} when the loss is strongly-convex or exp-concave in the first argument. 
\begin{lemma}
	\label{lem:translation}
	Let $\alpha >0$. If $\ell\colon \cH \times \cZ \rightarrow [-1,1]$ is $\alpha$-strongly-convex in the first argument, then $\ell(\cdot, z)$ is $\alpha$-exp-concave, for all $z\in \cZ$.  
	Furthermore, whenever $\ell\colon \cH \times \cZ \rightarrow [-1,1]$ is $\alpha$-exp-concave in the first argument, then the $(1, 4(1/\alpha+1))$-Bernstein condition holds. 
\end{lemma}

\begin{proof}{\bf of Theorem \ref{thm:bern}.\ }
	We will instantiate the bound of Theorem \ref{thm:VP} with $\hro= \mathcal{N}(\hat h^{\erm}_{\smallsetminus n}, \sigma^2\bm{I})$ and $\hro_{\smallsetminus i}= \mathcal{N}(\hat h_{\smallsetminus i}^{\erm}, \sigma^2\bm{I})$, for some $\sigma^2 >0$ which we will choose later. Using the fact that $(a + b)^2 \leq 2 a^2 + 2 b^2$, for all $a,b \in \reals$, and Jensen's inequality, we have
	\begin{align}
	\E_{Z^n}[\what V (\hro)] &\leq  \E_{Z^n} \left[\frac{2}{n}\sum_{i=1}^n  \E_{h\sim \hro}\left[(\ell(h,Z_i) - \ell(h^\star, Z_i))^2\right]  \right] \\& \quad +  \E_{Z^n} \left[\frac{2}{n}\sum_{i=1}^n \E_{h\sim \hro_{\smallsetminus i}} \left[(\ell(h,Z_i) - \ell(h^\star, Z_i))^2\right]  \right] , \\
	& \leq   \E_{Z^n} \left[\frac{2}{n}\sum_{i=1}^n  \E_{h\sim \hro}\left[(\ell(h,Z_i) - \ell(h^\star, Z_i))^2\right]  \right] \\& \quad + 2 \E_{Z^n} \left[ \E_{h\sim \hro_{\smallsetminus n}} \left[(\ell(h,Z_n) - \ell(h^\star, Z_n))^2\right]  \right],
	\end{align}
By another application of Theorem \ref{thm:VP} without leave-one-out, we have
	\begin{align}
	\E_{Z^n} \left[\frac{2}{n}\sum_{i=1}^n  \E_{h\sim \hro}\left[(\ell(h,Z_i) - \ell(h^\star, Z_i))^2\right]  \right] &	\leq   2\E_{Z}\left[ \E_{Z^n} \left[  \E_{h\sim \hro}\left[(\ell(h,Z) - \ell(h^\star, Z))^2\right]   \right]\right] \nonumber \\& \hspace{-3cm}  + \E_{Z^n} \left[\frac{2\eta \vartheta(\eta)}{n}\sum_{i=1}^n  \E_{h\sim \hro}\left[(\ell(h,Z_i) - \ell(h^\star, Z_i))^4 + \frac{\KL(\hro\| \ro_0)}{n}\right]  \right],\nonumber
\shortintertext{Since the losses are between 0 and 1, we get}
& \leq   2\E_{Z}\left[ \E_{Z^n} \left[  \E_{h\sim \hro}\left[(\ell(h,Z) - \ell(h^\star, Z))^2\right]   \right]\right] \nonumber \\& \hspace{-3cm} + \E_{Z^n} \left[\frac{2\eta \vartheta(\eta)}{n}\sum_{i=1}^n  \E_{h\sim \hro}\left[(\ell(h,Z_i) - \ell(h^\star, Z_i))^2 + \frac{\KL(\hro\| \ro_0)}{\eta n}\right]  \right], 
	\end{align}
By rearraging this inequality we get
\begin{align}
	(1-\eta\vartheta(\eta))\E_{Z^n} \left[\frac{2}{n}\sum_{i=1}^n  \E_{h\sim \hro}\left[(\ell(h,Z_i) - \ell(h^\star, Z_i))^2\right]  \right]& \leq \E_{Z^n} \left[\frac{\KL(\hro\| \ro_0)}{\eta n}\right] \\ 
	& \hspace{-2cm} 2\E_{Z^n} \left[  \E_{h\sim \hro}\left[(\ell(h,Z) - \ell(h^\star, Z))^2\right]   \right]
	\end{align}
	
		\todo[inline]{To be continued}
\end{proof}
}

\bibliography{biblio}
\bibliographystyle{plain}

\end{document}
